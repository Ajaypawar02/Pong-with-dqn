{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong with dqn",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNTSzuPhsoFa"
      },
      "source": [
        "import torch\n",
        "import gym\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "checkpoint_file = \"/content/sample_data/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAnrANQyne9m"
      },
      "source": [
        "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
        "    def __init__(self, env=None, repeat=4, clip_reward=False):\n",
        "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
        "        self.repeat = repeat\n",
        "        self.shape = env.observation_space.low.shape\n",
        "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
        "        self.clip_reward = clip_reward\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self.repeat):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if self.clip_reward:\n",
        "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
        "            t_reward += reward\n",
        "            idx = i % 2\n",
        "            self.frame_buffer[idx] = obs\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
        "        return max_frame, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "\n",
        "\n",
        "        self.frame_buffer = np.zeros_like((2,self.shape))\n",
        "        self.frame_buffer[0] = obs\n",
        "\n",
        "        return obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ5NWilxstG3"
      },
      "source": [
        "class RepeatActionMax(gym.Wrapper):\n",
        "  def __init__(self, repeat, env = None, clip_reward = False):\n",
        "    super(RepeatActionMax, self).__init__(env)\n",
        "    self.repeat = repeat\n",
        "    self.shape = env.observation_space.low.shape\n",
        "    self.max_frame = np.zeros_like((2, max_frame))\n",
        "    self.clip_reward = self.clip_reward\n",
        "  def step(self, action):\n",
        "    t_reward = 0\n",
        "    done = False\n",
        "    for i in range(self.repeat):\n",
        "      obs, reward, done , info = self.env.step()\n",
        "      if self.clip_reward:\n",
        "        reward = np.clip([reward], -1, 1)[0]\n",
        "      t_reward += reward\n",
        "      idx = i%2\n",
        "      self.max_frame[idx] = obs\n",
        "      if done:\n",
        "        break\n",
        "    maxframe = max(self.max_frame[0], self.max_frame[1])\n",
        "\n",
        "    return maxframe, t_reward, done , info\n",
        "\n",
        "  def reset(self):\n",
        "    obs = self.env.step()\n",
        "    self.max_frame = np.zeros_like((2, self.shape))\n",
        "    self.max_frame[0] = obs\n",
        "\n",
        "    return obs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzThNbTJykVI"
      },
      "source": [
        "class PreprocessFrame(gym.ObservationWrapper):\n",
        "  def __init__(self,shape, env = None):\n",
        "    super(PreprocessFrame, self).__init__(env)\n",
        "    self.shape = (shape[2], shape[0], shape[1])\n",
        "    self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32)\n",
        "\n",
        "  def observation(self, obs):\n",
        "    new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "    resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
        "                                    interpolation=cv2.INTER_AREA)\n",
        "    new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
        "    new_obs = new_obs / 255.0\n",
        "    \n",
        "    return new_obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvmYNJDr1CZA"
      },
      "source": [
        "class StackFrames(gym.ObservationWrapper):\n",
        "    def __init__(self, env, repeat):\n",
        "        super(StackFrames, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                            env.observation_space.low.repeat(repeat, axis=0),\n",
        "                            env.observation_space.high.repeat(repeat, axis=0),\n",
        "                            dtype=np.float32)\n",
        "        self.stack = collections.deque(maxlen=repeat)\n",
        "\n",
        "    def reset(self):\n",
        "        self.stack.clear()\n",
        "        #print(\"Noo\")\n",
        "        observation = self.env.reset()\n",
        "        #print(observation.shape)\n",
        "        for _ in range(self.stack.maxlen):\n",
        "            self.stack.append(observation)\n",
        "\n",
        "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.stack.append(observation)\n",
        "\n",
        "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,\n",
        "             no_ops=0, fire_first=False):\n",
        "    env = gym.make(env_name)\n",
        "    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards)\n",
        "    env = PreprocessFrame(shape, env)\n",
        "    env = StackFrames(env, repeat)\n",
        "\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv4wMLiZUh7O"
      },
      "source": [
        "class DeepQNetwork(nn.Module):\n",
        "  def __init__(self, lr, n_actions ,name, input_dims):\n",
        "    super(DeepQNetwork, self).__init__()\n",
        "    self.lr = lr\n",
        "    self.conv1 = nn.Conv2d(input_dims[0], 32, 8, stride=4)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "    self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "\n",
        "    flatten = self.calculate_dims(input_dims)\n",
        "\n",
        "    self.fc1 = nn.Linear(flatten, 512)\n",
        "    self.fc2 = nn.Linear(512, n_actions)\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr = lr)\n",
        "    self.loss = nn.MSELoss()\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "\n",
        "\n",
        "  def calculate_dims(self, input_dims):\n",
        "    state = T.zeros(1, *input_dims)\n",
        "    dims = self.conv1(state)\n",
        "    dims = self.conv2(dims)\n",
        "    dims = self.conv3(dims)\n",
        "\n",
        "    return int(np.prod(dims.size()))\n",
        "\n",
        "  def forward(self, state):\n",
        "    layer1 = F.relu(self.conv1(state))\n",
        "    layer2 = F.relu(self.conv2(layer1))\n",
        "    layer3 = F.relu(self.conv3(layer2))\n",
        "    conv_state = layer3.view(layer3.size()[0], -1)\n",
        "    flat_1 = F.relu(self.fc1(conv_state))\n",
        "    actions = self.fc2(flat_1)\n",
        "\n",
        "    return actions\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_BD4o3KSpFB"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, gamma, epsilon , lr, n_actions,input_dims,mem_size , batch_size,eps_min = 0.01,eps_dec = 5e-7,\n",
        "                env_name = None):\n",
        "    self.lr = lr\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = gamma\n",
        "    self.eps_dec = eps_dec\n",
        "    self.mem_size = mem_size\n",
        "    self.eps_min = eps_min\n",
        "    self.input_dims = input_dims\n",
        "    self.env_name = env_name \n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "    self.action_space = [i for i in range(n_actions)]\n",
        "    self.mem_cntr = 0\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
        "    self.action_memory = np.zeros(self.mem_size, dtype = np.int32)\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype = np.bool)\n",
        "\n",
        "    self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
        "                                    input_dims=self.input_dims,\n",
        "                                    name=self.env_name)\n",
        "\n",
        "    self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
        "                                    input_dims=self.input_dims,\n",
        "                                    name=self.env_name)\n",
        "\n",
        "\n",
        "  def store_transition(self,state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        #print(index)\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.reward_memory[index] = reward\n",
        "        self.action_memory[index] = action\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "        self.step_counter = 0\n",
        "\n",
        "  def choose_actions(self ,observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = T.tensor([observation],dtype= T.float32).to(self.q_eval.device)\n",
        "            \n",
        "            actions = self.q_eval.forward(state)\n",
        "            action = T.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space) \n",
        "\n",
        "        return action\n",
        "  def learn(self):\n",
        "       if self.mem_cntr < self.batch_size:\n",
        "         return\n",
        "       self.q_eval.optimizer.zero_grad()\n",
        "\n",
        "       if self.step_counter % 1000 == 0:\n",
        "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
        "\n",
        "       max_mem = min(self.mem_cntr, self.mem_size)\n",
        "       batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
        "       batch_index = np.arange(self.batch_size, dtype = np.int32)\n",
        "       state_batch = T.tensor(self.state_memory[batch]).to(self.q_eval.device)\n",
        "       new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.q_eval.device)\n",
        "       reward_batch = T.tensor(self.reward_memory[batch]).to(self.q_eval.device)\n",
        "       terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.q_eval.device)\n",
        "       action_batch = self.action_memory[batch]\n",
        "\n",
        "       q_eval = self.q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "       q_next = self.q_next.forward(new_state_batch)\n",
        "       q_next[terminal_batch] = 0.0\n",
        "\n",
        "       q_target = reward_batch + self.gamma * T.max(q_next, dim = 1)[0]\n",
        "\n",
        "       loss = self.q_eval.loss(q_target,q_eval).to(self.q_eval.device)\n",
        "       loss.backward()\n",
        "       self.q_eval.optimizer.step()\n",
        "       self.step_counter += 1\n",
        "\n",
        "       self.epsilon  = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
        "                        else self.eps_min\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR-Ff0GqFqE"
      },
      "source": [
        "def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOM54fLRU0Vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139a82a2-a2ec-430c-bd73-e9276458516f"
      },
      "source": [
        "env = make_env('PongNoFrameskip-v4')\n",
        "best_score = -np.inf\n",
        "load_checkpoint = False\n",
        "n_epsiodes = 500\n",
        "\n",
        "agent = Agent(gamma=0.99, epsilon=1, lr=0.0001,\n",
        "                     input_dims=(env.observation_space.shape),\n",
        "                     n_actions=env.action_space.n, mem_size=20000, eps_min=0.1,\n",
        "                     batch_size=32, eps_dec=1e-5,env_name = 'Pong' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "erarg5D-pCPy",
        "outputId": "6fd1d5c9-a2f7-4880-c2b9-39d3ac63cec2"
      },
      "source": [
        "n_steps = 0\n",
        "scores, eps_history, steps_array = [], [], []\n",
        "load_checkpoint = False\n",
        "for i in range(n_epsiodes):\n",
        "  done = False\n",
        "  observation = env.reset()\n",
        "  #print(observation.shape)\n",
        "  score = 0\n",
        "  while not done:\n",
        "\n",
        "    #print(observation.shape)\n",
        "    action = agent.choose_actions(observation)\n",
        "    observation_, reward, done, info = env.step(action)\n",
        "    #print(observation_.shape)\n",
        "    score += reward\n",
        "    if not load_checkpoint:\n",
        "      agent.store_transition(observation, action,\n",
        "                                     reward, observation_, done)\n",
        "      agent.learn()\n",
        "    observation = observation_\n",
        "    n_steps += 1\n",
        "  scores.append(score)\n",
        "  steps_array.append(n_steps)\n",
        "  avg_score = np.mean(scores[-100:])\n",
        "  print('episode: ', i,'score: ', score,\n",
        "             ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
        "            'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
        "  \n",
        "  if avg_score > best_score:\n",
        "    best_score = avg_score\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode:  0 score:  -18.0  average score -18.0 best score -21.00 epsilon 0.98 steps 1161\n",
            "episode:  1 score:  -20.0  average score -19.0 best score -18.00 epsilon 0.97 steps 2110\n",
            "episode:  2 score:  -18.0  average score -18.7 best score -18.00 epsilon 0.96 steps 3430\n",
            "episode:  3 score:  -21.0  average score -19.2 best score -18.00 epsilon 0.95 steps 4194\n",
            "episode:  4 score:  -21.0  average score -19.6 best score -18.00 epsilon 0.94 steps 5046\n",
            "episode:  5 score:  -20.0  average score -19.7 best score -18.00 epsilon 0.93 steps 5888\n",
            "episode:  6 score:  -21.0  average score -19.9 best score -18.00 epsilon 0.93 steps 6652\n",
            "episode:  7 score:  -21.0  average score -20.0 best score -18.00 epsilon 0.92 steps 7478\n",
            "episode:  8 score:  -21.0  average score -20.1 best score -18.00 epsilon 0.91 steps 8383\n",
            "episode:  9 score:  -18.0  average score -19.9 best score -18.00 epsilon 0.90 steps 9455\n",
            "episode:  10 score:  -20.0  average score -19.9 best score -18.00 epsilon 0.89 steps 10357\n",
            "episode:  11 score:  -21.0  average score -20.0 best score -18.00 epsilon 0.88 steps 11149\n",
            "episode:  12 score:  -20.0  average score -20.0 best score -18.00 epsilon 0.87 steps 11993\n",
            "episode:  13 score:  -20.0  average score -20.0 best score -18.00 epsilon 0.86 steps 13003\n",
            "episode:  14 score:  -21.0  average score -20.1 best score -18.00 epsilon 0.85 steps 13767\n",
            "episode:  15 score:  -21.0  average score -20.1 best score -18.00 epsilon 0.85 steps 14647\n",
            "episode:  16 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.84 steps 15537\n",
            "episode:  17 score:  -19.0  average score -20.1 best score -18.00 epsilon 0.82 steps 16757\n",
            "episode:  18 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.82 steps 17670\n",
            "episode:  19 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.81 steps 18583\n",
            "episode:  20 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.80 steps 19407\n",
            "episode:  21 score:  -19.0  average score -20.2 best score -18.00 epsilon 0.79 steps 20562\n",
            "episode:  22 score:  -19.0  average score -20.1 best score -18.00 epsilon 0.78 steps 21649\n",
            "episode:  23 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.77 steps 22654\n",
            "episode:  24 score:  -20.0  average score -20.2 best score -18.00 epsilon 0.76 steps 23575\n",
            "episode:  25 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.75 steps 24472\n",
            "episode:  26 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.74 steps 25315\n",
            "episode:  27 score:  -20.0  average score -20.2 best score -18.00 epsilon 0.73 steps 26324\n",
            "episode:  28 score:  -21.0  average score -20.2 best score -18.00 epsilon 0.72 steps 27148\n",
            "episode:  29 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.71 steps 27959\n",
            "episode:  30 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.70 steps 28723\n",
            "episode:  31 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.70 steps 29561\n",
            "episode:  32 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.69 steps 30385\n",
            "episode:  33 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.68 steps 31330\n",
            "episode:  34 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.67 steps 32284\n",
            "episode:  35 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.66 steps 33170\n",
            "episode:  36 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.65 steps 34022\n",
            "episode:  37 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.64 steps 34903\n",
            "episode:  38 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.63 steps 35787\n",
            "episode:  39 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.62 steps 36824\n",
            "episode:  40 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.62 steps 37648\n",
            "episode:  41 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.61 steps 38612\n",
            "episode:  42 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.60 steps 39609\n",
            "episode:  43 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.58 steps 40758\n",
            "episode:  44 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.57 steps 41750\n",
            "episode:  45 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.57 steps 42570\n",
            "episode:  46 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.55 steps 43706\n",
            "episode:  47 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.55 steps 44576\n",
            "episode:  48 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.54 steps 45359\n",
            "episode:  49 score:  -18.0  average score -20.4 best score -18.00 epsilon 0.53 steps 46451\n",
            "episode:  50 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.52 steps 47336\n",
            "episode:  51 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.51 steps 48206\n",
            "episode:  52 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.50 steps 49076\n",
            "episode:  53 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.49 steps 49974\n",
            "episode:  54 score:  -17.0  average score -20.3 best score -18.00 epsilon 0.48 steps 51111\n",
            "episode:  55 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.47 steps 51995\n",
            "episode:  56 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.46 steps 52838\n",
            "episode:  57 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.45 steps 53704\n",
            "episode:  58 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.45 steps 54566\n",
            "episode:  59 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.44 steps 55404\n",
            "episode:  60 score:  -19.0  average score -20.3 best score -18.00 epsilon 0.43 steps 56401\n",
            "episode:  61 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.42 steps 57422\n",
            "episode:  62 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.41 steps 58303\n",
            "episode:  63 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.40 steps 59095\n",
            "episode:  64 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.39 steps 59921\n",
            "episode:  65 score:  -21.0  average score -20.3 best score -18.00 epsilon 0.38 steps 60763\n",
            "episode:  66 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.37 steps 61693\n",
            "episode:  67 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.37 steps 62577\n",
            "episode:  68 score:  -20.0  average score -20.3 best score -18.00 epsilon 0.36 steps 63420\n",
            "episode:  69 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.35 steps 64184\n",
            "episode:  70 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.34 steps 65008\n",
            "episode:  71 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.33 steps 65912\n",
            "episode:  72 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.32 steps 66824\n",
            "episode:  73 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.31 steps 67840\n",
            "episode:  74 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.31 steps 68666\n",
            "episode:  75 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.30 steps 69599\n",
            "episode:  76 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.29 steps 70511\n",
            "episode:  77 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.28 steps 71465\n",
            "episode:  78 score:  -19.0  average score -20.4 best score -18.00 epsilon 0.27 steps 72489\n",
            "episode:  79 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.26 steps 73253\n",
            "episode:  80 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.25 steps 74079\n",
            "episode:  81 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.24 steps 74933\n",
            "episode:  82 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.23 steps 75811\n",
            "episode:  83 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.22 steps 76820\n",
            "episode:  84 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.22 steps 77612\n",
            "episode:  85 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.21 steps 78590\n",
            "episode:  86 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.20 steps 79574\n",
            "episode:  87 score:  -19.0  average score -20.4 best score -18.00 epsilon 0.18 steps 80772\n",
            "episode:  88 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.17 steps 81756\n",
            "episode:  89 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.16 steps 82844\n",
            "episode:  90 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.15 steps 83756\n",
            "episode:  91 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.15 steps 84675\n",
            "episode:  92 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.14 steps 85576\n",
            "episode:  93 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.13 steps 86368\n",
            "episode:  94 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.12 steps 87439\n",
            "episode:  95 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.11 steps 88399\n",
            "episode:  96 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.10 steps 89242\n",
            "episode:  97 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 90090\n",
            "episode:  98 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.10 steps 91039\n",
            "episode:  99 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.10 steps 91993\n",
            "episode:  100 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 92785\n",
            "episode:  101 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 93577\n",
            "episode:  102 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 94403\n",
            "episode:  103 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 95229\n",
            "episode:  104 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 96223\n",
            "episode:  105 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.10 steps 97121\n",
            "episode:  106 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 98039\n",
            "episode:  107 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 98863\n",
            "episode:  108 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 99751\n",
            "episode:  109 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 100577\n",
            "episode:  110 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 101433\n",
            "episode:  111 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 102295\n",
            "episode:  112 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 103121\n",
            "episode:  113 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 104102\n",
            "episode:  114 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 105048\n",
            "episode:  115 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 106059\n",
            "episode:  116 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 107101\n",
            "episode:  117 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 108047\n",
            "episode:  118 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 109160\n",
            "episode:  119 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 110183\n",
            "episode:  120 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 111129\n",
            "episode:  121 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 112078\n",
            "episode:  122 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 112930\n",
            "episode:  123 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 113756\n",
            "episode:  124 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 114684\n",
            "episode:  125 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 115644\n",
            "episode:  126 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 116532\n",
            "episode:  127 score:  -21.0  average score -20.5 best score -18.00 epsilon 0.10 steps 117420\n",
            "episode:  128 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 118368\n",
            "episode:  129 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 119246\n",
            "episode:  130 score:  -19.0  average score -20.5 best score -18.00 epsilon 0.10 steps 120439\n",
            "episode:  131 score:  -18.0  average score -20.5 best score -18.00 epsilon 0.10 steps 121733\n",
            "episode:  132 score:  -20.0  average score -20.5 best score -18.00 epsilon 0.10 steps 123268\n",
            "episode:  133 score:  -19.0  average score -20.4 best score -18.00 epsilon 0.10 steps 124799\n",
            "episode:  134 score:  -19.0  average score -20.4 best score -18.00 epsilon 0.10 steps 125856\n",
            "episode:  135 score:  -17.0  average score -20.4 best score -18.00 epsilon 0.10 steps 127604\n",
            "episode:  136 score:  -18.0  average score -20.4 best score -18.00 epsilon 0.10 steps 129278\n",
            "episode:  137 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 131028\n",
            "episode:  138 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.10 steps 132692\n",
            "episode:  139 score:  -20.0  average score -20.4 best score -18.00 epsilon 0.10 steps 133805\n",
            "episode:  140 score:  -21.0  average score -20.4 best score -18.00 epsilon 0.10 steps 135241\n",
            "episode:  141 score:  -19.0  average score -20.4 best score -18.00 epsilon 0.10 steps 137003\n",
            "episode:  142 score:  -18.0  average score -20.3 best score -18.00 epsilon 0.10 steps 138693\n",
            "episode:  143 score:  -19.0  average score -20.3 best score -18.00 epsilon 0.10 steps 140178\n",
            "episode:  144 score:  -16.0  average score -20.3 best score -18.00 epsilon 0.10 steps 142776\n",
            "episode:  145 score:  -19.0  average score -20.2 best score -18.00 epsilon 0.10 steps 144635\n",
            "episode:  146 score:  -20.0  average score -20.2 best score -18.00 epsilon 0.10 steps 146917\n",
            "episode:  147 score:  -20.0  average score -20.2 best score -18.00 epsilon 0.10 steps 148572\n",
            "episode:  148 score:  -18.0  average score -20.2 best score -18.00 epsilon 0.10 steps 150623\n",
            "episode:  149 score:  -17.0  average score -20.2 best score -18.00 epsilon 0.10 steps 152795\n",
            "episode:  150 score:  -20.0  average score -20.2 best score -18.00 epsilon 0.10 steps 154769\n",
            "episode:  151 score:  -18.0  average score -20.2 best score -18.00 epsilon 0.10 steps 157010\n",
            "episode:  152 score:  -19.0  average score -20.1 best score -18.00 epsilon 0.10 steps 159173\n",
            "episode:  153 score:  -20.0  average score -20.1 best score -18.00 epsilon 0.10 steps 161121\n",
            "episode:  154 score:  -17.0  average score -20.1 best score -18.00 epsilon 0.10 steps 162937\n",
            "episode:  155 score:  -20.0  average score -20.1 best score -18.00 epsilon 0.10 steps 164676\n",
            "episode:  156 score:  -12.0  average score -20.1 best score -18.00 epsilon 0.10 steps 167105\n",
            "episode:  157 score:  -19.0  average score -20.0 best score -18.00 epsilon 0.10 steps 169113\n",
            "episode:  158 score:  -20.0  average score -20.0 best score -18.00 epsilon 0.10 steps 171197\n",
            "episode:  159 score:  -14.0  average score -20.0 best score -18.00 epsilon 0.10 steps 173312\n",
            "episode:  160 score:  -19.0  average score -20.0 best score -18.00 epsilon 0.10 steps 175039\n",
            "episode:  161 score:  -19.0  average score -20.0 best score -18.00 epsilon 0.10 steps 177321\n",
            "episode:  162 score:  -19.0  average score -19.9 best score -18.00 epsilon 0.10 steps 179018\n",
            "episode:  163 score:  -16.0  average score -19.9 best score -18.00 epsilon 0.10 steps 180876\n",
            "episode:  164 score:  -17.0  average score -19.9 best score -18.00 epsilon 0.10 steps 182823\n",
            "episode:  165 score:  -18.0  average score -19.8 best score -18.00 epsilon 0.10 steps 184699\n",
            "episode:  166 score:  -11.0  average score -19.7 best score -18.00 epsilon 0.10 steps 187542\n",
            "episode:  167 score:  -16.0  average score -19.7 best score -18.00 epsilon 0.10 steps 189835\n",
            "episode:  168 score:  -17.0  average score -19.6 best score -18.00 epsilon 0.10 steps 191874\n",
            "episode:  169 score:  -19.0  average score -19.6 best score -18.00 epsilon 0.10 steps 193492\n",
            "episode:  170 score:  -16.0  average score -19.6 best score -18.00 epsilon 0.10 steps 195875\n",
            "episode:  171 score:  -13.0  average score -19.5 best score -18.00 epsilon 0.10 steps 198231\n",
            "episode:  172 score:  -15.0  average score -19.4 best score -18.00 epsilon 0.10 steps 200393\n",
            "episode:  173 score:  -15.0  average score -19.4 best score -18.00 epsilon 0.10 steps 202262\n",
            "episode:  174 score:  -19.0  average score -19.4 best score -18.00 epsilon 0.10 steps 204302\n",
            "episode:  175 score:  -17.0  average score -19.4 best score -18.00 epsilon 0.10 steps 206163\n",
            "episode:  176 score:  -17.0  average score -19.3 best score -18.00 epsilon 0.10 steps 208177\n",
            "episode:  177 score:  -12.0  average score -19.2 best score -18.00 epsilon 0.10 steps 211328\n",
            "episode:  178 score:  -17.0  average score -19.2 best score -18.00 epsilon 0.10 steps 213339\n",
            "episode:  179 score:  -18.0  average score -19.2 best score -18.00 epsilon 0.10 steps 215350\n",
            "episode:  180 score:  -11.0  average score -19.1 best score -18.00 epsilon 0.10 steps 218117\n",
            "episode:  181 score:  -18.0  average score -19.1 best score -18.00 epsilon 0.10 steps 219990\n",
            "episode:  182 score:  -16.0  average score -19.0 best score -18.00 epsilon 0.10 steps 222554\n",
            "episode:  183 score:  -14.0  average score -18.9 best score -18.00 epsilon 0.10 steps 225116\n",
            "episode:  184 score:  -11.0  average score -18.8 best score -18.00 epsilon 0.10 steps 228014\n",
            "episode:  185 score:  -18.0  average score -18.8 best score -18.00 epsilon 0.10 steps 230282\n",
            "episode:  186 score:  -14.0  average score -18.8 best score -18.00 epsilon 0.10 steps 232702\n",
            "episode:  187 score:  -16.0  average score -18.7 best score -18.00 epsilon 0.10 steps 235231\n",
            "episode:  188 score:  -19.0  average score -18.7 best score -18.00 epsilon 0.10 steps 236927\n",
            "episode:  189 score:  -15.0  average score -18.6 best score -18.00 epsilon 0.10 steps 239548\n",
            "episode:  190 score:  -20.0  average score -18.6 best score -18.00 epsilon 0.10 steps 241488\n",
            "episode:  191 score:  -13.0  average score -18.6 best score -18.00 epsilon 0.10 steps 243870\n",
            "episode:  192 score:  -9.0  average score -18.4 best score -18.00 epsilon 0.10 steps 246745\n",
            "episode:  193 score:  -16.0  average score -18.4 best score -18.00 epsilon 0.10 steps 249024\n",
            "episode:  194 score:  -14.0  average score -18.3 best score -18.00 epsilon 0.10 steps 251260\n",
            "episode:  195 score:  -19.0  average score -18.3 best score -18.00 epsilon 0.10 steps 252818\n",
            "episode:  196 score:  -15.0  average score -18.3 best score -18.00 epsilon 0.10 steps 254843\n",
            "episode:  197 score:  -14.0  average score -18.2 best score -18.00 epsilon 0.10 steps 257027\n",
            "episode:  198 score:  -14.0  average score -18.1 best score -18.00 epsilon 0.10 steps 259199\n",
            "episode:  199 score:  -15.0  average score -18.1 best score -18.00 epsilon 0.10 steps 261576\n",
            "episode:  200 score:  -17.0  average score -18.1 best score -18.00 epsilon 0.10 steps 263675\n",
            "episode:  201 score:  -15.0  average score -18.0 best score -18.00 epsilon 0.10 steps 265935\n",
            "episode:  202 score:  -10.0  average score -17.9 best score -18.00 epsilon 0.10 steps 268748\n",
            "episode:  203 score:  -12.0  average score -17.8 best score -17.89 epsilon 0.10 steps 271407\n",
            "episode:  204 score:  -16.0  average score -17.8 best score -17.80 epsilon 0.10 steps 273413\n",
            "episode:  205 score:  -15.0  average score -17.7 best score -17.75 epsilon 0.10 steps 275306\n",
            "episode:  206 score:  -16.0  average score -17.6 best score -17.70 epsilon 0.10 steps 277301\n",
            "episode:  207 score:  -15.0  average score -17.6 best score -17.65 epsilon 0.10 steps 279950\n",
            "episode:  208 score:  -15.0  average score -17.5 best score -17.59 epsilon 0.10 steps 282154\n",
            "episode:  209 score:  -14.0  average score -17.5 best score -17.53 epsilon 0.10 steps 284562\n",
            "episode:  210 score:  -16.0  average score -17.4 best score -17.46 epsilon 0.10 steps 286740\n",
            "episode:  211 score:  -15.0  average score -17.4 best score -17.41 epsilon 0.10 steps 289131\n",
            "episode:  212 score:  -13.0  average score -17.3 best score -17.36 epsilon 0.10 steps 291556\n",
            "episode:  213 score:  -14.0  average score -17.2 best score -17.28 epsilon 0.10 steps 293792\n",
            "episode:  214 score:  -13.0  average score -17.1 best score -17.22 epsilon 0.10 steps 296232\n",
            "episode:  215 score:  -8.0  average score -17.0 best score -17.14 epsilon 0.10 steps 299193\n",
            "episode:  216 score:  -11.0  average score -16.9 best score -17.02 epsilon 0.10 steps 302010\n",
            "episode:  217 score:  -13.0  average score -16.9 best score -16.93 epsilon 0.10 steps 304308\n",
            "episode:  218 score:  -15.0  average score -16.8 best score -16.85 epsilon 0.10 steps 306478\n",
            "episode:  219 score:  -10.0  average score -16.7 best score -16.80 epsilon 0.10 steps 309223\n",
            "episode:  220 score:  -11.0  average score -16.6 best score -16.69 epsilon 0.10 steps 311610\n",
            "episode:  221 score:  -16.0  average score -16.5 best score -16.59 epsilon 0.10 steps 313525\n",
            "episode:  222 score:  -16.0  average score -16.5 best score -16.54 epsilon 0.10 steps 315508\n",
            "episode:  223 score:  -6.0  average score -16.3 best score -16.49 epsilon 0.10 steps 318514\n",
            "episode:  224 score:  -11.0  average score -16.2 best score -16.34 epsilon 0.10 steps 321536\n",
            "episode:  225 score:  -14.0  average score -16.2 best score -16.24 epsilon 0.10 steps 324149\n",
            "episode:  226 score:  -6.0  average score -16.0 best score -16.17 epsilon 0.10 steps 327180\n",
            "episode:  227 score:  -12.0  average score -15.9 best score -16.02 epsilon 0.10 steps 329702\n",
            "episode:  228 score:  -10.0  average score -15.8 best score -15.93 epsilon 0.10 steps 332320\n",
            "episode:  229 score:  -9.0  average score -15.7 best score -15.83 epsilon 0.10 steps 334985\n",
            "episode:  230 score:  -11.0  average score -15.6 best score -15.72 epsilon 0.10 steps 337586\n",
            "episode:  231 score:  -9.0  average score -15.6 best score -15.64 epsilon 0.10 steps 340443\n",
            "episode:  232 score:  -7.0  average score -15.4 best score -15.55 epsilon 0.10 steps 343106\n",
            "episode:  233 score:  -11.0  average score -15.3 best score -15.42 epsilon 0.10 steps 345677\n",
            "episode:  234 score:  -8.0  average score -15.2 best score -15.34 epsilon 0.10 steps 348489\n",
            "episode:  235 score:  -8.0  average score -15.1 best score -15.23 epsilon 0.10 steps 351217\n",
            "episode:  236 score:  -10.0  average score -15.1 best score -15.14 epsilon 0.10 steps 353939\n",
            "episode:  237 score:  -13.0  average score -15.0 best score -15.06 epsilon 0.10 steps 356046\n",
            "episode:  238 score:  -7.0  average score -14.8 best score -14.98 epsilon 0.10 steps 359070\n",
            "episode:  239 score:  -9.0  average score -14.7 best score -14.85 epsilon 0.10 steps 361824\n",
            "episode:  240 score:  -5.0  average score -14.6 best score -14.74 epsilon 0.10 steps 365016\n",
            "episode:  241 score:  -8.0  average score -14.5 best score -14.58 epsilon 0.10 steps 368002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-6dddd56c8797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m       agent.store_transition(observation, action,\n\u001b[1;32m     18\u001b[0m                                      reward, observation_, done)\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-14fdc4eedb9a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m        \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m        \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m        \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m        \u001b[0mq_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterminal_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6xzLWiTqa2N"
      },
      "source": [
        "  figure_file = 'pong'+'.png'\n",
        "  x = [i+1 for i in range(len(scores))]\n",
        "  plot_learning_curve(steps_array, scores, eps_history, figure_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDfTHsMszj1e"
      },
      "source": [
        "agent.q_eval.state_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu3Tze-mzlyO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}